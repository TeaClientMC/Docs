---
title: "ðŸ¤– Robots.txt"
---
import YoutubeEmbed from "../../components/YoutubeEmbed.astro"








This file is used to give instructions to web robots, such as search engine crawlers, about which pages of the website should be processed or ignored.

## User-agents

Each `User-agent` line is followed by a `Disallow` line, which tells the specified robot not to crawl any pages.


## Crawling Cooldown

The `Crawl-delay` directive is used to prevent the server from being overloaded by requests. It tells the robot to wait a specified number of seconds between successive requests. In this case, the delay is set to 10 seconds.



## Other Directives

The `Disallow` directive is used to tell all other bots not listed to not crawl the specified pages. The `Noindex` directive is used to tell these bots not to index these pages.


## Sitemap

The `Sitemap` directive is used to specify the location of the XML sitemap.


## Note

Not all crawlers respect the "Crawl-delay" directive. Major search engines like Google, Bing, and Yahoo do, but some others might not. If you're having issues with a specific crawler, you might need to contact them directly or block them using the "Disallow" directive. 

Also, be aware that setting a high crawl delay can slow down the rate at which search engines index your site, which might not be desirable if your site updates frequently. 

# More Help


<YoutubeEmbed
   URL="https://www.youtube.com/embed/qRlQ965pGCA?si=5v8YKI1cYShHoR2y" />
Credits to: Rank Math SEO